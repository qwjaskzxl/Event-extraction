sklearn.svc 参数

C: float参数 默认值为1.0

错误项的惩罚系数。C越大，即对分错样本的惩罚程度越大，因此在训练样本中准确率越高，但是泛化能力降低，也就是对测试数据的分类准确率降低。相反，减小C的话，容许训练样本中有一些误分类错误样本，泛化能力强。对于训练样本带有噪声的情况，一般采用后者，把训练样本集中错误分类的样本作为噪声。

kernel: str参数 默认为‘rbf’

算法中采用的核函数类型，可选参数有：

‘linear’:线性核函数

‘poly’：多项式核函数

‘rbf’：径像核函数/高斯核

‘sigmod’:sigmod核函数

‘precomputed’:核矩阵

precomputed表示自己提前计算好核函数矩阵，这时候算法内部就不再用核函数去计算核矩阵，而是直接用你给的核矩阵。

degree: int型参数 默认为3

这个参数只对多项式核函数有用，是指多项式核函数的阶数n

如果给的核函数参数是其他核函数，则会自动忽略该参数。

gamma: float参数 默认为auto

核函数系数，只对‘rbf’,‘poly’,‘sigmod’有效。

如果gamma为auto，代表其值为样本特征数的倒数，即1/n_features.

coef0: float参数 默认为0.0

核函数中的独立项，只有对‘poly’和‘sigmod’核函数有用，是指其中的参数c

probability： bool参数 默认为False

是否启用概率估计。 这必须在调用fit()之前启用，并且会fit()方法速度变慢。


调参

线性核函数明显比其他三种好，但是理论上讲RBF是明显优于线性核函数的，对于分类问题，我们主要

需要调节两个超参数（惩罚系数和RBF核函数的系数）

惩罚系数即松弛变量的系数。它在优化函数里主要是平衡支持向量的复杂度和误分类率这两者之间的关系，可以理解为正则化系数。当比较大时，我们的损失函数也会越大，这意味着我们不愿意放弃比较远的离群点。这样我们会有更加多的支持向量，也就是说支持向量和超平面的模型也会变得越复杂，也容易过拟合。反之，当比较小时，意味我们不想理那些离群点，会选择较少的样本来做支持向量，最终的支持向量和超平面的模型也会简单。scikit-learn中默认值是1。

另一个超参数是RBF核函数的参数。主要定义了单个样本对整个分类超平面的影响，当比较大时，单个样本对整个分类超平面的影响比较小，不容易被选择为支持向量，或者说整个模型的支持向量也会少。

调参方法
由于经验不足，采取网格搜索的方法进行地毯式搜索
在python中建立笛卡尔积列表，将两种超参数进行组合，然后在模型中，选择一个拟合分数最好的超平面系数。同时打印每个输出的结果，人工进行分析比较，最终得出C=[2-5],gamma=[0.2,0.3,0.4]中的效果明显好于其他组合，也达到了可以与线性核函数媲美的效果，说明整体调参思路正确。

分析：C>1，说明模型对于错误样本的惩罚会高，对于训练集的拟合效果会更好，为什么泛化效果也优，可能是因为我们的测试机与训练集来自于同一文本，gamma大小偏小，说明每个样本对于超平面的影响是比较大的。


